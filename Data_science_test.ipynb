{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import BertTokenizer,AdamW,BertPreTrainedModel,BertModel,get_linear_schedule_with_warmup,BertConfig\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('input/Entity_sentiment_trainV2.xlsx')\n",
    "df_test = pd.read_excel('input/Entity_sentiment_testV2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The website was very easy to use and my insura...</td>\n",
       "      <td>website</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The web sight was easy to understand and I got...</td>\n",
       "      <td>web sight</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Having filled in the application on-line I cou...</td>\n",
       "      <td>point</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After finding AXA was cheaper than my renewal ...</td>\n",
       "      <td>prices</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The quote was a reasonable price compared with...</td>\n",
       "      <td>insurances</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence      Entity Sentiment\n",
       "0  The website was very easy to use and my insura...     website  positive\n",
       "1  The web sight was easy to understand and I got...   web sight  positive\n",
       "2  Having filled in the application on-line I cou...       point  negative\n",
       "3  After finding AXA was cheaper than my renewal ...      prices  positive\n",
       "4  The quote was a reasonable price compared with...  insurances  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ps bruce if you've won the euromillions don't forget to pay my years worth off\"\n",
      "'years'\n",
      "'negative'\n"
     ]
    }
   ],
   "source": [
    "idx = random.choice(range(len(df)))\n",
    "pprint(df.Sentence.iloc[idx])\n",
    "pprint(df.Entity.iloc[idx])\n",
    "pprint(df.Sentiment.iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence     0\n",
       "Entity       0\n",
       "Sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    4100\n",
       "negative    1899\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the length of Entity Tokens\n",
    "#[(i,t)  for i,t in enumerate(df.Entity) if len(t.split()) !=1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function 1- config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class config:\n",
    "    MAX_LEN = 128\n",
    "    TRAIN_BATCH_SIZE = 32\n",
    "    VALID_BATCH_SIZE = 16\n",
    "    EPOCHS = 2\n",
    "    BERT_PATH = \"../input/bert-base-uncased/\"\n",
    "    MODEL_PATH = \"pytorch_model.bin\"\n",
    "    print('Loading BERT tokenizer...')\n",
    "    TOKENIZER = BertTokenizer.from_pretrained(f\"{BERT_PATH}/vocab.txt\", do_lower_case=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"max_len = 0\\nsentences = df.Sentence.tolist() + df_test.Sentence.tolist()\\n\\n# For every sentence...\\nfor sent in sentences:\\n\\n    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\\n    input_ids = config.TOKENIZER.encode(sent, add_special_tokens=True)\\n\\n    # Update the maximum sentence length.\\n    max_len = max(max_len, len(input_ids))\\n\\nprint('Max sentence length: ', max_len)\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get maximu length\n",
    "\"\"\"max_len = 0\n",
    "sentences = df.Sentence.tolist() + df_test.Sentence.tolist()\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = config.TOKENIZER.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.MAX_LEN = 155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_col(x):\n",
    "    #print(x)\n",
    "    sentence = config.TOKENIZER.tokenize(x['Sentence'])\n",
    "    entity = config.TOKENIZER.tokenize(x['Entity'])\n",
    "    ent_index = []\n",
    "    for ent in entity:\n",
    "        try:\n",
    "            ent_index.append(str(sentence.index(ent) + 1))\n",
    "        except:\n",
    "            pass\n",
    "    if len(ent_index) > 0:\n",
    "        return ' '.join(ent_index)\n",
    "\n",
    "    return '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(sentence, entity, sentiment,entity_indexes, tokenizer, max_len):\n",
    "    \n",
    "    \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sentence,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = max_len,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True   # Construct attn. masks.\n",
    "                                       )\n",
    "    \n",
    "    input_ids = encoded_dict['input_ids']\n",
    "    \n",
    "    mask = encoded_dict['attention_mask']\n",
    "    \n",
    "    label = 1 if sentiment == 'positive' else 0\n",
    "    \n",
    "\n",
    "    return {\n",
    "        'ids': input_ids,\n",
    "        'mask': mask,\n",
    "        'entity_indexes':entity_indexes,\n",
    "        'target': label,\n",
    "        'sentence': sentence,\n",
    "        'entity': entity\n",
    "       \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = process_data(df.Sentence.iloc[7], df.Entity.iloc[7], df.Sentiment.iloc[7],df.entity_indexes.iloc[7], config.TOKENIZER, config.MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset:\n",
    "    def __init__(self, sentence, entity, sentiment,entity_indexes):\n",
    "        self.sentence = sentence\n",
    "        self.entity = entity\n",
    "        self.sentiment = sentiment\n",
    "        self.entity_indexes = entity_indexes\n",
    "        self.tokenizer = config.TOKENIZER\n",
    "        self.max_len = config.MAX_LEN\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentence)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        data = process_data(\n",
    "            self.sentence[item], \n",
    "            self.entity[item], \n",
    "            self.sentiment[item],\n",
    "            self.entity_indexes[item],\n",
    "            self.tokenizer,\n",
    "            self.max_len\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
    "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
    "            'target': torch.tensor(data[\"target\"]),\n",
    "            'entity_indexes':data['entity_indexes'],\n",
    "            'sentence':data['sentence'],\n",
    "            'entity':data['entity']\n",
    "        }\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = SentimentDataset(df.Sentence.tolist(), df.Entity.tolist(), df.Sentiment.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntitySentimentModel(BertPreTrainedModel):\n",
    "    def __init__(self, conf):\n",
    "        super(EntitySentimentModel, self).__init__(conf)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.bert = BertModel.from_pretrained(config.BERT_PATH, config=conf)\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(768, 1)\n",
    "        torch.nn.init.normal_(self.classifier.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, ids, mask,ent_indexes,labels = None):\n",
    "        \n",
    "        _, _, out = self.bert(\n",
    "            ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=None\n",
    "        )\n",
    "        \n",
    "        last_hidden_state = out[-1]\n",
    "        #print('Hidden State Shape-----',last_hidden_state.shape)\n",
    "        #print('Ent Inx shape-----------',len(ent_indexes))\n",
    "        \n",
    "        ent_word_embeds = []\n",
    "        #for i,ent_index in enumerate()\n",
    "        for i,idx in enumerate(ent_indexes):\n",
    "            ent_embedds = []\n",
    "            ent_idx = [int(t) for t in idx.split()]\n",
    "            for j in ent_idx:\n",
    "                ent_embedds.append(last_hidden_state[i,j,:].squeeze())\n",
    "            ent_embedds = torch.mean(torch.stack(ent_embedds,1),1)\n",
    "            #print('last try',ent_embedds.shape)\n",
    "            ent_word_embeds.append(ent_embedds)\n",
    "            #ent_word_embeds[i,:] = ent_embedds.unsqueeze(0)\n",
    "        \n",
    "        ent_word_embeds = torch.stack(ent_word_embeds)\n",
    "        #print('Concat Shape----------',ent_word_embeds.shape)\n",
    "        ent_word_embeds = self.drop_out(ent_word_embeds)\n",
    "        #print('Dropout Shape----------',ent_word_embeds.shape)\n",
    "        logits = self.classifier(ent_word_embeds).squeeze()\n",
    "        \n",
    "        if labels is not None:\n",
    "            #print('*******',labels)\n",
    "            #print('---------',logits.shape)\n",
    "            labels = labels.type_as(logits)\n",
    "            #print('hiiiiii',labels[:1])\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            return loss\n",
    "      \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    \n",
    "    for bi, d in enumerate(tk0):\n",
    "\n",
    "        ids = d[\"ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        target = d[\"target\"]\n",
    "        entity_indexes = d[\"entity_indexes\"]\n",
    "        sentence = d[\"sentence\"]\n",
    "        entity = d[\"entity\"]\n",
    "        \n",
    "        #print('Iterating on first data point')\n",
    "        #print(entity_indexes)\n",
    "  \n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        target = target.to(device, dtype=torch.long)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss = model(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            ent_indexes = entity_indexes,\n",
    "            labels=target\n",
    "        )\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(data_loader) \n",
    "    print(f'Average Loss at  is {avg_train_loss}')\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation Code\n",
    "\n",
    "def accuracy_params(pred,true):\n",
    "    pred_classes = [1 if p>0.5 else 0 for p in pred]\n",
    "    \n",
    "    accuracy = accuracy_score(true, pred_classes)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(true, pred_classes)\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(true, pred_classes)\n",
    "    print('Recall: %f' % recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(true, pred_classes)\n",
    "    print('F1 score: %f' % f1)\n",
    "    \n",
    "\n",
    "def eval_fn(data_loader, model, device,test = False):\n",
    "    model.eval()\n",
    "    test_predictions = []\n",
    "    true = []\n",
    "    with torch.no_grad():\n",
    "        tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "        for bi, d in enumerate(tk0):\n",
    "            ids = d[\"ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            target = d[\"target\"]\n",
    "            entity_indexes = d[\"entity_indexes\"]\n",
    "            sentence = d[\"sentence\"]\n",
    "            entity = d[\"entity\"]\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            target = target.to(device, dtype=torch.long)\n",
    "\n",
    "            logits = model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                ent_indexes = entity_indexes\n",
    "            )\n",
    "            \n",
    "            target = target.type_as(logits)\n",
    "            \n",
    "            test_predictions.append(logits.cpu().detach().numpy())\n",
    "            true.append(target.cpu().detach().numpy())\n",
    "            \n",
    "            loss = nn.BCEWithLogitsLoss()(logits, target)\n",
    "            print('Loss during Validation Data',loss)\n",
    "            \n",
    "        test_predictions = np.concatenate( test_predictions, axis=0 ) \n",
    "        true = np.concatenate(true, axis=0 )\n",
    "        \n",
    "        test_predictions = torch.sigmoid(torch.tensor(test_predictions)).numpy()\n",
    "        \n",
    "        accuracy_params(test_predictions,true)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(df_train,df_valid = None,fold = None):\n",
    "    #dfx = pd.read_csv(config.TRAINING_FILE)\n",
    "\n",
    "    #df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n",
    "    #df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = SentimentDataset(\n",
    "        sentence=df_train.Sentence.values,\n",
    "        entity=df_train.Entity.values,\n",
    "        sentiment=df_train.Sentiment.values,\n",
    "        entity_indexes = df_train.entity_indexes.values\n",
    "    )\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.TRAIN_BATCH_SIZE,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    valid_dataset = SentimentDataset(\n",
    "        sentence=df_valid.Sentence.values,\n",
    "        entity=df_valid.Entity.values,\n",
    "        sentiment=df_valid.Sentiment.values,\n",
    "        entity_indexes = df_valid.entity_indexes.values\n",
    "    )\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=config.VALID_BATCH_SIZE,\n",
    "        shuffle = False,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    model_config = BertConfig.from_pretrained(config.BERT_PATH)\n",
    "    model_config.output_hidden_states = True\n",
    "    model = EntitySentimentModel(conf=model_config)\n",
    "    model.to(device)\n",
    "\n",
    "    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=0, \n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    # I'm training only for 2 epochs and saving the model on last iteration\n",
    "    for epoch in range(3):\n",
    "        train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n",
    "        eval_fn(valid_data_loader, model, device)\n",
    "        \n",
    "        if epoch == 2:\n",
    "            torch.save(model.state_dict(), f'fold_{fold}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function 2 - train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5a6da3fe2b449184360422939aadd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=150.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Loss at  is 0.2859835806985696\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f594d27e6f4e3fab6a8d67f8d64e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=75.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss during Validation Data tensor(0.2677, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0215, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.3378, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2631, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1872, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1317, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1360, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2116, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.4272, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1225, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1414, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0847, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0541, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0783, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0376, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0474, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2134, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1610, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0190, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2281, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2863, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0235, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1971, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1389, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1700, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0988, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1490, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.3459, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0424, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1422, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1271, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1038, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0614, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0690, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.3352, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1645, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.3473, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1253, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0971, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2598, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1744, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0961, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.5872, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1603, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2111, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0647, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1545, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1387, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1573, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1829, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1947, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.7073, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1133, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0889, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0504, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0409, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1832, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0635, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1745, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1719, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2303, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2018, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.3110, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2137, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0369, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1159, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2413, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.4197, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0403, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2173, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1918, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0881, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.4052, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1846, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0560, device='cuda:0')\n",
      "\n",
      "Accuracy: 0.936667\n",
      "Precision: 0.950363\n",
      "Recall: 0.957317\n",
      "F1 score: 0.953827\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eedf27eb1da74b39a85a5c640333b840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=150.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Loss at  is 0.1224664031714201\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13f47d24e104b00bdcb8156178fe960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=75.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss during Validation Data tensor(0.3098, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0078, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2456, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2537, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2418, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0672, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1119, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1697, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.4367, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1407, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1225, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0210, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0248, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0293, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0171, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0264, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2577, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1655, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0066, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.3359, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.4290, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0067, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2038, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1268, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1103, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0547, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1544, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.4624, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0117, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1940, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0643, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0708, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0366, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0208, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.4086, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1182, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.4062, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0675, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0877, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2518, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1864, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0601, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.6582, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0970, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2283, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0372, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1409, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1565, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1033, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2792, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2418, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.8057, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0960, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1301, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0174, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0136, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1818, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1496, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1631, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1276, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2813, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2407, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.3793, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1182, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0136, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1328, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2692, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.6359, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0257, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1802, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1694, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0806, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.4714, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1275, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0356, device='cuda:0')\n",
      "\n",
      "Accuracy: 0.935000\n",
      "Precision: 0.950243\n",
      "Recall: 0.954878\n",
      "F1 score: 0.952555\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8383758703984d52a66e77f0614fca81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=150.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Loss at  is 0.08771276794994871\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04baed29c33f414ead87cba9d1dcf050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=75.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss during Validation Data tensor(0.3098, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0078, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2456, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2537, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2418, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0672, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1119, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1697, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.4367, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1407, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1225, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0210, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0248, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0293, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0171, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0264, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2577, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1655, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0066, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.3359, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.4290, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0067, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2038, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1268, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1103, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0547, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1544, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.4624, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0117, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1940, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0643, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0708, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0366, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0208, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.4086, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1182, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.4062, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0675, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0877, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2518, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1864, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0601, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.6582, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0970, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2283, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0372, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1409, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1565, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1033, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2792, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2418, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.8057, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0960, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1301, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0174, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0136, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1818, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1496, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1631, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1276, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2813, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2407, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.3793, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1182, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0136, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1328, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2692, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.6359, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0257, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1802, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1694, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0806, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.4714, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1275, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0356, device='cuda:0')\n",
      "\n",
      "Accuracy: 0.935000\n",
      "Precision: 0.950243\n",
      "Recall: 0.954878\n",
      "F1 score: 0.952555\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b29a137dfdf4115be0e27d9ba7c957f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=150.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Loss at  is 0.2659868985414505\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51dc785bd32a4d95a24090e5913507a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=75.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss during Validation Data tensor(0.2777, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1645, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0912, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0411, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2470, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1489, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2336, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2246, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1003, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0788, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.3668, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0352, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0322, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0670, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0337, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1434, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0864, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0869, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1493, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1725, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.3512, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1672, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0932, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2535, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.3641, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.3287, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.4374, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0205, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2599, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0564, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0999, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0544, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0661, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.3499, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.3763, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0640, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.3234, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0358, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1816, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0696, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2937, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2812, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0888, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.4083, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0399, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1669, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1149, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.3865, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.3162, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0569, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.3375, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0256, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.3204, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2899, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0416, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.3213, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.6486, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0733, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2092, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2493, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1498, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2693, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0887, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.3590, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1429, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0330, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1081, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2788, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2953, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0619, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0756, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.1725, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.4216, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.0663, device='cuda:0')\n",
      "Loss during Validation Data tensor(0.2202, device='cuda:0')\n",
      "\n",
      "Accuracy: 0.925833\n",
      "Precision: 0.951792\n",
      "Recall: 0.939024\n",
      "F1 score: 0.945365\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d862cea31674e0398acfc81cb2a3aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=150.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-04d254c6d5a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-0b6f7589cdf5>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(df_train, df_valid, fold)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# I'm training only for 3 epochs even though I specified 5!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0meval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-d6a4c938cdf4>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(data_loader, model, optimizer, device, scheduler)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         )\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(df):\n",
    "    df['entity_indexes'] = df.apply(format_col,axis = 1)\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n",
    "    i=0\n",
    "    for train_index,valid_index in kf.split(df.Sentence,df.Sentiment):\n",
    "        df_train,df_valid = df.loc[train_index],df.loc[valid_index]\n",
    "        run(df_train,df_valid,i)\n",
    "        i +=1\n",
    "        \n",
    "train_model(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function 3- test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EntitySentimentModel(\n",
       "  (loss_fn): BCEWithLogitsLoss()\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (drop_out): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_model(df_test)\n",
    "    device = torch.device(\"cuda\")\n",
    "    model_config = BertConfig.from_pretrained(config.BERT_PATH)\n",
    "    model_config.output_hidden_states = True\n",
    "\n",
    "    model1 = EntitySentimentModel(conf=model_config)\n",
    "    model1.to(device)\n",
    "    model1.load_state_dict(torch.load(\"fold_0\"))\n",
    "    model1.eval()\n",
    "    \n",
    "\n",
    "    df_test.loc[:, \"Sentiment\"] = 0\n",
    "    df_test['entity_indexes'] = df_test.apply(format_col,axis = 1)\n",
    "\n",
    "\n",
    "    test_dataset = SentimentDataset(\n",
    "        sentence=df_test.Sentence.values,\n",
    "        entity=df_test.Entity.values,\n",
    "        sentiment=df_test.Sentiment.values,\n",
    "        entity_indexes = df_test.entity_indexes.values\n",
    "    )\n",
    "\n",
    "    test_data_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        shuffle = False,\n",
    "        batch_size=config.VALID_BATCH_SIZE,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    test_predictions = []\n",
    "    with torch.no_grad():\n",
    "        tk0 = tqdm(test_data_loader, total=len(test_data_loader))\n",
    "        for bi, d in enumerate(tk0):\n",
    "            ids = d[\"ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            target = d[\"target\"]\n",
    "            entity_indexes = d[\"entity_indexes\"]\n",
    "            sentence = d[\"sentence\"]\n",
    "            entity = d[\"entity\"]\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            target = target.to(device, dtype=torch.long)\n",
    "\n",
    "            logits = model1(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                ent_indexes = entity_indexes\n",
    "            )\n",
    "\n",
    "            target = target.type_as(logits)\n",
    "\n",
    "            test_predictions.append(logits.cpu().detach().numpy())\n",
    "\n",
    "    test_predictions = np.concatenate( test_predictions, axis=0 ) \n",
    "        \n",
    "    test_predictions = torch.sigmoid(torch.tensor(test_predictions)).numpy()\n",
    "    pred = [1 if p>0.5 else 0 for p in test_predictions]\n",
    "    pred = ['positive' if p == 1 else 'negative' for p in pred]\n",
    "    df_test.loc[:,'Sentiment'] = pred\n",
    "    df_test = df_test[['Sentence','Entity','Sentiment']]\n",
    "    \n",
    "    return df_test\n",
    " \n",
    "df_test = test_model(df_test)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Taken out 2 car insurance policies today went through policies with customer '\n",
      " 'advisor Paul who was very helpful and explained details to me so i could '\n",
      " 'easily understand.')\n",
      "'customer advisor'\n",
      "'positive'\n"
     ]
    }
   ],
   "source": [
    "idx = random.choice(range(len(df_test)))\n",
    "pprint(df_test.Sentence.iloc[idx])\n",
    "pprint(df_test.Entity.iloc[idx])\n",
    "pprint(df_test.Sentiment.iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('test_pred.csv',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
